{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mascj670/Advanced-CC-week-10/blob/main/gpt_3_fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTaP4We_UBRb"
      },
      "source": [
        "# Fine Tune GPT-3 #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPZhJ-ZdUBRc"
      },
      "source": [
        "Additional training to make the model better at specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojdpp93sUBRc"
      },
      "source": [
        "Run the following on your terminal to prepare data. Type \"y\" to advance through all the options\n",
        "\n",
        "openai tools fine_tunes.prepare_data -f love_movie.csv\n",
        "\n",
        "Note: Make sure you're running this in the same directory as the love_movie.csv file\n",
        "\n",
        "Data was prepared from this article: https://www.empireonline.com/movies/features/best-romantic-movies/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVWbQKdhUBRc"
      },
      "source": [
        "You want to convert data so it looks something like this\n",
        "\n",
        "```\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXO3LWcCmfRK",
        "outputId": "9694d43d-458a-4c7f-a27f-eb5e562bed42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "openai.api_key = \"\""
      ],
      "metadata": {
        "id": "BdesWd7omhBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "OoVY8rH0qkpm",
        "outputId": "185d6d3d-3bf5-44e2-ae29-f484f17ef5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-15e3978e-83ed-4101-bbb5-045c9ac80d31\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-15e3978e-83ed-4101-bbb5-045c9ac80d31\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving love_movie.csv to love_movie.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbKTFvWUUBRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3aaf873-3690-4272-a6d7-5bf80a0b7456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Based on your file extension, your file is formatted as a CSV file\n",
            "- Your file contains 60 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
            "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
            "- Your data does not contain a common ending at the end of your completions. Having a common ending string appended to the end of the completion makes it clearer to the fine-tuned model where the completion should end. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples.\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
            "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: Y\n",
            "- [Recommended] Add a suffix ending `\\n` to all completions [Y/n]: Y\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: y\n",
            "\n",
            "Wrote modified file to `love_movie_prepared.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"love_movie_prepared.jsonl\"\n",
            "\n",
            "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"\\n\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 3.27 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ],
      "source": [
        "# run this in terminal (in same directory as the csv file)\n",
        "# respond Y to all 3 qestions\n",
        "# it may take a few mins to run\n",
        "!openai tools fine_tunes.prepare_data -f love_movie.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env OPENAI_API_KEY=INSERT_API_KEY_HERE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMKyWd95sQLj",
        "outputId": "2ca9e74e-b7b7-4b4b-ff03-b4a869690cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OPENAI_API_KEY=INSERT_API_KEY_HERE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umKOH_DBUBRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35536625-5949-41fe-f2d6-a118e9ad4d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found potentially duplicated files with name 'love_movie_prepared.jsonl', purpose 'fine-tune' and size 30199 bytes\n",
            "file-r1JLUFj3FHEuwb5JhF52ksm8\n",
            "file-hf7eZ93mroqpihTIDVc3bZ8n\n",
            "Enter file ID to reuse an already uploaded file, or an empty string to upload this file anyway: \n",
            "Upload progress: 100% 30.2k/30.2k [00:00<00:00, 59.3Mit/s]\n",
            "Uploaded file from love_movie_prepared.jsonl: file-5TTdMStEKqPLpSO3Ofzt7YGO\n",
            "Created fine-tune: ft-sSW2oSz3YU9S5QcrDBtuc7mf\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2023-09-17 20:06:49] Created fine-tune: ft-sSW2oSz3YU9S5QcrDBtuc7mf\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-sSW2oSz3YU9S5QcrDBtuc7mf\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#run the following, you may get interrupted.\n",
        "#Follow instructions to keep running until the model is completed\n",
        "#If stream interrupted (client disconnected).\n",
        "#To resume the stream, run: openai api fine_tunes.follow -i fine-tune-id\n",
        "#It will take a few mins\n",
        "\n",
        "!openai api fine_tunes.create -t love_movie_prepared.jsonl -m ada\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.follow -i ft-sSW2oSz3YU9S5QcrDBtuc7mf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4OT7tY9weSn",
        "outputId": "d07250d8-690f-4910-c6f8-94f1d648627f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-09-17 20:06:49] Created fine-tune: ft-sSW2oSz3YU9S5QcrDBtuc7mf\n",
            "[2023-09-17 20:16:12] Fine-tune costs $0.01\n",
            "[2023-09-17 20:16:13] Fine-tune enqueued. Queue number: 0\n",
            "[2023-09-17 20:16:14] Fine-tune started\n",
            "[2023-09-17 20:16:36] Completed epoch 1/4\n",
            "[2023-09-17 20:16:46] Completed epoch 2/4\n",
            "[2023-09-17 20:16:55] Completed epoch 3/4\n",
            "[2023-09-17 20:17:04] Completed epoch 4/4\n",
            "[2023-09-17 20:17:26] Uploaded model: ada:ft-personal-2023-09-17-20-17-25\n",
            "[2023-09-17 20:17:26] Uploaded result file: file-i4NJcHVJ1clTSl7m5o32BkJv\n",
            "[2023-09-17 20:17:26] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded 🎉\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal-2023-09-17-20-17-25 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"INSERT_API_KEY_HERE\""
      ],
      "metadata": {
        "id": "UkJIrbpNvoLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVBsba6-UBRf"
      },
      "outputs": [],
      "source": [
        "#look at model results\n",
        "response = openai.Completion.create(\n",
        "    model=\"ada:ft-personal-2023-09-17-20-17-25\",\n",
        "    prompt=\"Gone Girl ->\", #recommend to add\n",
        "    temperature=0.4, #randomness\n",
        "    max_tokens=120, #charaters in response\n",
        "    top_p=1, #controls diversity\n",
        "    stop=[\"\\n\"], #recommended to add\n",
        "    frequency_penalty=0, #decrease repetition\n",
        "    presence_penalty=0 #increase likelihood to talk about new topic\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC6dODtKUBRf",
        "outputId": "047bf732-504b-4f62-fb8f-6bf63adc3f5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " It’s hard to find fault with Julia (Zooey Deschanel), a single, successful writer who is visited by a ghostly, elderly woman (Claire Foy) who offers her a place in her attic home. But there’s more to her story than meets the eye. The film’s emotional core is the relationship between her husband, Frank (Zooey Deschanel), and the woman he’s sleeping with. It’s a great love story that’s also a great love story.\n"
          ]
        }
      ],
      "source": [
        "result = response.choices[0].text\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4dJ9Q9CUBRg"
      },
      "outputs": [],
      "source": [
        "#compare same prompt to general model\n",
        "response = openai.Completion.create(\n",
        "  model=\"text-davinci-002\",\n",
        "  prompt=\"Gone Girl\",\n",
        "  temperature=0.4,\n",
        "  max_tokens=100,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKKroW8mUBRg",
        "outputId": "bc0835be-fd27-4e77-c812-cb62e3be4dd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "” — David Fincher\n",
            "\n",
            "“The Grand Budapest Hotel” — Wes Anderson\n",
            "\n",
            "“Birdman” — Alejandro G. Iñárritu\n",
            "\n",
            "“The Imitation Game” — Morten Tyldum\n",
            "\n",
            "“Selma” — Ava DuVernay\n",
            "\n",
            "“The Theory of Everything” — James Marsh\n",
            "\n",
            "“Whiplash” — Damien Chazelle\n",
            "\n",
            "B\n"
          ]
        }
      ],
      "source": [
        "result = response.choices[0].text\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDscB-hlUBRg"
      },
      "outputs": [],
      "source": [
        "#to delete model run the following\n",
        "openai.Model.delete(\"FINE_TUNED_MODEL\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}